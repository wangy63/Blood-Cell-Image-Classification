{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class to load data into the network\n",
    "class mydata(data.Dataset):\n",
    "    def __init__(self, image_folder_path):\n",
    "        # image_folder_path is the train folder, validation folder, or test folder\n",
    "        \n",
    "        # folder of the images\n",
    "        self.root_dir = image_folder_path\n",
    "        \n",
    "        # set up a list to hold the full path for each image\n",
    "        self.image_names = []\n",
    "        \n",
    "        # set up a list to hold the labels of the images\n",
    "        self.labels = []\n",
    "        \n",
    "        # obtain image file path and its label\n",
    "        for wbc_type in os.listdir(self.root_dir):\n",
    "            '''\n",
    "            nn.CrossEntropyLoss expects logits in the shape of [batch_size, number_of_classes]\n",
    "            and a target tensor of [batch_size] with class indices as its values.\n",
    "            '''\n",
    "            if not wbc_type.startswith('.'):\n",
    "                if wbc_type in ['NEUTROPHIL']: \n",
    "                    label = 0 #label of neutrophil\n",
    "                elif wbc_type in ['LYMPHOCYTE']: \n",
    "                    label = 1 #label of lymphocyte\n",
    "                elif wbc_type in ['MONOCYTE']: \n",
    "                    label = 2 #label of monocyte\n",
    "                elif wbc_type in ['EOSINOPHIL']:\n",
    "                    label = 3 #label of eosinophil\n",
    "                \n",
    "                for image_filename in os.listdir(os.path.join(self.root_dir, wbc_type)):\n",
    "                    self.image_names.append(os.path.join(self.root_dir, wbc_type, image_filename))\n",
    "                    self.labels.append(label)\n",
    "               \n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_names[index])\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # convert the RGB image to tensor and normalize the tensor for loading into resnet-18\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        image_norm = preprocess(image)\n",
    "        \n",
    "        label_tensor = torch.tensor(label)\n",
    "        #label_tensor = label_tensor.to(dtype=torch.long)\n",
    "                \n",
    "        return image_norm, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS AVAILABLE!\n",
      "epoch: 0 - (75 seconds) - train loss: 635.838 - valid loss: 36.766 - train accuracy: 0.47 - valid accuracy: 0.345\n",
      "epoch: 1 - (74 seconds) - train loss: 582.903 - valid loss: 35.079 - train accuracy: 0.564 - valid accuracy: 0.436\n",
      "epoch: 2 - (74 seconds) - train loss: 545.586 - valid loss: 30.167 - train accuracy: 0.649 - valid accuracy: 0.6\n",
      "epoch: 3 - (74 seconds) - train loss: 505.191 - valid loss: 31.636 - train accuracy: 0.748 - valid accuracy: 0.6\n",
      "epoch: 4 - (74 seconds) - train loss: 481.32 - valid loss: 29.582 - train accuracy: 0.799 - valid accuracy: 0.6\n",
      "epoch: 5 - (74 seconds) - train loss: 457.92 - valid loss: 31.81 - train accuracy: 0.849 - valid accuracy: 0.473\n",
      "epoch: 6 - (74 seconds) - train loss: 451.2 - valid loss: 27.002 - train accuracy: 0.856 - valid accuracy: 0.8\n",
      "epoch: 7 - (74 seconds) - train loss: 429.661 - valid loss: 27.862 - train accuracy: 0.896 - valid accuracy: 0.727\n",
      "epoch: 8 - (74 seconds) - train loss: 420.68 - valid loss: 26.493 - train accuracy: 0.917 - valid accuracy: 0.818\n",
      "epoch: 9 - (74 seconds) - train loss: 419.819 - valid loss: 26.095 - train accuracy: 0.912 - valid accuracy: 0.782\n",
      "epoch: 10 - (74 seconds) - train loss: 411.818 - valid loss: 25.72 - train accuracy: 0.922 - valid accuracy: 0.855\n",
      "epoch: 11 - (74 seconds) - train loss: 414.275 - valid loss: 24.839 - train accuracy: 0.923 - valid accuracy: 0.855\n",
      "epoch: 12 - (74 seconds) - train loss: 409.376 - valid loss: 25.302 - train accuracy: 0.941 - valid accuracy: 0.8\n",
      "epoch: 13 - (74 seconds) - train loss: 403.163 - valid loss: 24.56 - train accuracy: 0.94 - valid accuracy: 0.873\n",
      "epoch: 14 - (74 seconds) - train loss: 404.414 - valid loss: 26.626 - train accuracy: 0.935 - valid accuracy: 0.818\n",
      "epoch: 15 - (74 seconds) - train loss: 400.54 - valid loss: 24.997 - train accuracy: 0.947 - valid accuracy: 0.836\n",
      "epoch: 16 - (74 seconds) - train loss: 403.17 - valid loss: 24.954 - train accuracy: 0.942 - valid accuracy: 0.855\n",
      "epoch: 17 - (74 seconds) - train loss: 403.146 - valid loss: 25.002 - train accuracy: 0.938 - valid accuracy: 0.836\n",
      "epoch: 18 - (74 seconds) - train loss: 396.262 - valid loss: 25.265 - train accuracy: 0.949 - valid accuracy: 0.855\n",
      "epoch: 19 - (74 seconds) - train loss: 395.28 - valid loss: 25.963 - train accuracy: 0.957 - valid accuracy: 0.8\n",
      "epoch: 20 - (74 seconds) - train loss: 395.204 - valid loss: 25.427 - train accuracy: 0.957 - valid accuracy: 0.855\n",
      "epoch: 21 - (74 seconds) - train loss: 389.345 - valid loss: 24.745 - train accuracy: 0.971 - valid accuracy: 0.855\n",
      "epoch: 22 - (74 seconds) - train loss: 393.405 - valid loss: 25.107 - train accuracy: 0.96 - valid accuracy: 0.855\n",
      "epoch: 23 - (74 seconds) - train loss: 393.82 - valid loss: 23.789 - train accuracy: 0.96 - valid accuracy: 0.909\n",
      "epoch: 24 - (74 seconds) - train loss: 389.099 - valid loss: 23.982 - train accuracy: 0.969 - valid accuracy: 0.855\n",
      "epoch: 25 - (74 seconds) - train loss: 388.595 - valid loss: 25.1 - train accuracy: 0.968 - valid accuracy: 0.818\n",
      "epoch: 26 - (74 seconds) - train loss: 382.138 - valid loss: 25.004 - train accuracy: 0.987 - valid accuracy: 0.873\n",
      "epoch: 27 - (74 seconds) - train loss: 386.602 - valid loss: 25.496 - train accuracy: 0.967 - valid accuracy: 0.8\n",
      "epoch: 28 - (74 seconds) - train loss: 383.672 - valid loss: 23.95 - train accuracy: 0.977 - valid accuracy: 0.873\n",
      "epoch: 29 - (74 seconds) - train loss: 388.268 - valid loss: 23.914 - train accuracy: 0.968 - valid accuracy: 0.891\n",
      "epoch: 30 - (74 seconds) - train loss: 381.465 - valid loss: 24.161 - train accuracy: 0.981 - valid accuracy: 0.855\n",
      "epoch: 31 - (74 seconds) - train loss: 397.571 - valid loss: 24.171 - train accuracy: 0.947 - valid accuracy: 0.873\n",
      "epoch: 32 - (74 seconds) - train loss: 387.894 - valid loss: 24.93 - train accuracy: 0.968 - valid accuracy: 0.855\n",
      "epoch: 33 - (74 seconds) - train loss: 382.305 - valid loss: 25.42 - train accuracy: 0.983 - valid accuracy: 0.818\n",
      "epoch: 34 - (74 seconds) - train loss: 388.876 - valid loss: 24.51 - train accuracy: 0.962 - valid accuracy: 0.873\n",
      "epoch: 35 - (74 seconds) - train loss: 377.382 - valid loss: 24.995 - train accuracy: 0.992 - valid accuracy: 0.836\n",
      "epoch: 36 - (74 seconds) - train loss: 383.959 - valid loss: 28.509 - train accuracy: 0.98 - valid accuracy: 0.709\n",
      "epoch: 37 - (74 seconds) - train loss: 381.429 - valid loss: 25.223 - train accuracy: 0.982 - valid accuracy: 0.855\n",
      "epoch: 38 - (74 seconds) - train loss: 380.703 - valid loss: 24.705 - train accuracy: 0.981 - valid accuracy: 0.855\n",
      "epoch: 39 - (74 seconds) - train loss: 383.648 - valid loss: 24.381 - train accuracy: 0.978 - valid accuracy: 0.891\n",
      "epoch: 40 - (74 seconds) - train loss: 380.497 - valid loss: 24.357 - train accuracy: 0.986 - valid accuracy: 0.855\n",
      "epoch: 41 - (74 seconds) - train loss: 380.233 - valid loss: 24.546 - train accuracy: 0.981 - valid accuracy: 0.855\n",
      "epoch: 42 - (74 seconds) - train loss: 376.367 - valid loss: 25.396 - train accuracy: 0.992 - valid accuracy: 0.836\n",
      "epoch: 43 - (74 seconds) - train loss: 378.248 - valid loss: 24.173 - train accuracy: 0.989 - valid accuracy: 0.873\n",
      "epoch: 44 - (74 seconds) - train loss: 386.489 - valid loss: 26.144 - train accuracy: 0.97 - valid accuracy: 0.8\n",
      "epoch: 45 - (75 seconds) - train loss: 381.161 - valid loss: 23.87 - train accuracy: 0.979 - valid accuracy: 0.855\n",
      "epoch: 46 - (74 seconds) - train loss: 383.03 - valid loss: 25.118 - train accuracy: 0.982 - valid accuracy: 0.836\n",
      "epoch: 47 - (74 seconds) - train loss: 374.804 - valid loss: 24.439 - train accuracy: 0.996 - valid accuracy: 0.855\n",
      "epoch: 48 - (74 seconds) - train loss: 378.088 - valid loss: 24.687 - train accuracy: 0.992 - valid accuracy: 0.855\n",
      "epoch: 49 - (74 seconds) - train loss: 382.834 - valid loss: 24.539 - train accuracy: 0.979 - valid accuracy: 0.855\n",
      "epoch: 50 - (74 seconds) - train loss: 378.602 - valid loss: 23.855 - train accuracy: 0.988 - valid accuracy: 0.891\n",
      "epoch: 51 - (74 seconds) - train loss: 375.47 - valid loss: 23.674 - train accuracy: 0.992 - valid accuracy: 0.873\n",
      "epoch: 52 - (74 seconds) - train loss: 372.926 - valid loss: 23.838 - train accuracy: 0.999 - valid accuracy: 0.891\n",
      "epoch: 53 - (74 seconds) - train loss: 386.587 - valid loss: 24.388 - train accuracy: 0.97 - valid accuracy: 0.855\n",
      "epoch: 54 - (74 seconds) - train loss: 376.167 - valid loss: 24.756 - train accuracy: 0.991 - valid accuracy: 0.836\n",
      "epoch: 55 - (74 seconds) - train loss: 373.798 - valid loss: 23.284 - train accuracy: 0.997 - valid accuracy: 0.891\n",
      "epoch: 56 - (75 seconds) - train loss: 381.359 - valid loss: 25.733 - train accuracy: 0.982 - valid accuracy: 0.836\n",
      "epoch: 57 - (74 seconds) - train loss: 379.335 - valid loss: 23.993 - train accuracy: 0.985 - valid accuracy: 0.873\n",
      "epoch: 58 - (74 seconds) - train loss: 372.713 - valid loss: 24.554 - train accuracy: 0.999 - valid accuracy: 0.855\n",
      "epoch: 59 - (74 seconds) - train loss: 372.177 - valid loss: 23.629 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 60 - (74 seconds) - train loss: 373.472 - valid loss: 25.744 - train accuracy: 0.998 - valid accuracy: 0.836\n",
      "epoch: 61 - (74 seconds) - train loss: 387.649 - valid loss: 25.07 - train accuracy: 0.967 - valid accuracy: 0.836\n",
      "epoch: 62 - (74 seconds) - train loss: 376.631 - valid loss: 24.448 - train accuracy: 0.994 - valid accuracy: 0.873\n",
      "epoch: 63 - (74 seconds) - train loss: 377.031 - valid loss: 26.546 - train accuracy: 0.989 - valid accuracy: 0.764\n",
      "epoch: 64 - (74 seconds) - train loss: 377.199 - valid loss: 25.091 - train accuracy: 0.989 - valid accuracy: 0.855\n",
      "epoch: 65 - (74 seconds) - train loss: 375.747 - valid loss: 24.356 - train accuracy: 0.995 - valid accuracy: 0.855\n",
      "epoch: 66 - (75 seconds) - train loss: 374.836 - valid loss: 25.163 - train accuracy: 0.994 - valid accuracy: 0.855\n",
      "epoch: 67 - (74 seconds) - train loss: 378.541 - valid loss: 24.911 - train accuracy: 0.987 - valid accuracy: 0.855\n",
      "epoch: 68 - (74 seconds) - train loss: 372.315 - valid loss: 24.892 - train accuracy: 1.0 - valid accuracy: 0.855\n",
      "epoch: 69 - (74 seconds) - train loss: 372.06 - valid loss: 24.258 - train accuracy: 1.0 - valid accuracy: 0.873\n",
      "epoch: 70 - (74 seconds) - train loss: 376.963 - valid loss: 24.796 - train accuracy: 0.992 - valid accuracy: 0.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 71 - (74 seconds) - train loss: 384.125 - valid loss: 25.009 - train accuracy: 0.974 - valid accuracy: 0.836\n",
      "epoch: 72 - (74 seconds) - train loss: 373.249 - valid loss: 25.916 - train accuracy: 0.999 - valid accuracy: 0.836\n",
      "epoch: 73 - (74 seconds) - train loss: 372.052 - valid loss: 25.079 - train accuracy: 1.0 - valid accuracy: 0.836\n",
      "epoch: 74 - (74 seconds) - train loss: 372.116 - valid loss: 27.268 - train accuracy: 1.0 - valid accuracy: 0.727\n",
      "epoch: 75 - (74 seconds) - train loss: 389.214 - valid loss: 24.685 - train accuracy: 0.96 - valid accuracy: 0.836\n",
      "epoch: 76 - (74 seconds) - train loss: 374.129 - valid loss: 24.21 - train accuracy: 0.998 - valid accuracy: 0.873\n",
      "epoch: 77 - (74 seconds) - train loss: 374.424 - valid loss: 24.108 - train accuracy: 0.997 - valid accuracy: 0.873\n",
      "epoch: 78 - (74 seconds) - train loss: 376.656 - valid loss: 27.055 - train accuracy: 0.99 - valid accuracy: 0.764\n",
      "epoch: 79 - (74 seconds) - train loss: 378.315 - valid loss: 23.603 - train accuracy: 0.988 - valid accuracy: 0.891\n",
      "epoch: 80 - (74 seconds) - train loss: 373.119 - valid loss: 28.054 - train accuracy: 0.998 - valid accuracy: 0.691\n",
      "epoch: 81 - (74 seconds) - train loss: 381.806 - valid loss: 26.072 - train accuracy: 0.979 - valid accuracy: 0.8\n",
      "epoch: 82 - (75 seconds) - train loss: 377.151 - valid loss: 25.45 - train accuracy: 0.993 - valid accuracy: 0.818\n",
      "epoch: 83 - (74 seconds) - train loss: 372.089 - valid loss: 25.005 - train accuracy: 1.0 - valid accuracy: 0.836\n",
      "epoch: 84 - (74 seconds) - train loss: 374.223 - valid loss: 26.49 - train accuracy: 0.994 - valid accuracy: 0.782\n",
      "epoch: 85 - (74 seconds) - train loss: 375.543 - valid loss: 24.249 - train accuracy: 0.993 - valid accuracy: 0.855\n",
      "epoch: 86 - (74 seconds) - train loss: 375.441 - valid loss: 23.752 - train accuracy: 0.995 - valid accuracy: 0.873\n",
      "epoch: 87 - (74 seconds) - train loss: 372.238 - valid loss: 25.348 - train accuracy: 1.0 - valid accuracy: 0.818\n",
      "epoch: 88 - (74 seconds) - train loss: 372.019 - valid loss: 24.454 - train accuracy: 1.0 - valid accuracy: 0.873\n",
      "epoch: 89 - (74 seconds) - train loss: 371.91 - valid loss: 25.006 - train accuracy: 1.0 - valid accuracy: 0.855\n",
      "epoch: 90 - (74 seconds) - train loss: 371.882 - valid loss: 24.173 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 91 - (74 seconds) - train loss: 371.891 - valid loss: 24.085 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 92 - (74 seconds) - train loss: 371.947 - valid loss: 24.003 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 93 - (75 seconds) - train loss: 394.427 - valid loss: 24.526 - train accuracy: 0.953 - valid accuracy: 0.855\n",
      "epoch: 94 - (74 seconds) - train loss: 374.785 - valid loss: 26.84 - train accuracy: 0.995 - valid accuracy: 0.782\n",
      "epoch: 95 - (74 seconds) - train loss: 375.944 - valid loss: 24.749 - train accuracy: 0.994 - valid accuracy: 0.855\n",
      "epoch: 96 - (74 seconds) - train loss: 372.192 - valid loss: 24.509 - train accuracy: 1.0 - valid accuracy: 0.836\n",
      "epoch: 97 - (74 seconds) - train loss: 372.055 - valid loss: 24.823 - train accuracy: 1.0 - valid accuracy: 0.836\n",
      "epoch: 98 - (75 seconds) - train loss: 380.628 - valid loss: 25.76 - train accuracy: 0.984 - valid accuracy: 0.818\n",
      "epoch: 99 - (74 seconds) - train loss: 374.697 - valid loss: 24.041 - train accuracy: 0.996 - valid accuracy: 0.909\n",
      "epoch: 100 - (74 seconds) - train loss: 378.456 - valid loss: 25.484 - train accuracy: 0.985 - valid accuracy: 0.8\n",
      "epoch: 101 - (74 seconds) - train loss: 372.494 - valid loss: 22.961 - train accuracy: 1.0 - valid accuracy: 0.909\n",
      "epoch: 102 - (74 seconds) - train loss: 375.214 - valid loss: 26.067 - train accuracy: 0.995 - valid accuracy: 0.836\n",
      "epoch: 103 - (74 seconds) - train loss: 372.854 - valid loss: 24.839 - train accuracy: 0.999 - valid accuracy: 0.836\n",
      "epoch: 104 - (75 seconds) - train loss: 373.644 - valid loss: 28.761 - train accuracy: 0.998 - valid accuracy: 0.709\n",
      "epoch: 105 - (74 seconds) - train loss: 390.921 - valid loss: 23.603 - train accuracy: 0.962 - valid accuracy: 0.891\n",
      "epoch: 106 - (74 seconds) - train loss: 372.32 - valid loss: 24.221 - train accuracy: 1.0 - valid accuracy: 0.873\n",
      "epoch: 107 - (74 seconds) - train loss: 371.948 - valid loss: 24.7 - train accuracy: 1.0 - valid accuracy: 0.836\n",
      "epoch: 108 - (74 seconds) - train loss: 371.95 - valid loss: 24.647 - train accuracy: 1.0 - valid accuracy: 0.855\n",
      "epoch: 109 - (74 seconds) - train loss: 380.173 - valid loss: 24.236 - train accuracy: 0.984 - valid accuracy: 0.855\n",
      "epoch: 110 - (74 seconds) - train loss: 372.59 - valid loss: 26.135 - train accuracy: 0.999 - valid accuracy: 0.8\n",
      "epoch: 111 - (74 seconds) - train loss: 376.327 - valid loss: 24.287 - train accuracy: 0.99 - valid accuracy: 0.855\n",
      "epoch: 112 - (74 seconds) - train loss: 373.248 - valid loss: 23.824 - train accuracy: 0.999 - valid accuracy: 0.855\n",
      "epoch: 113 - (74 seconds) - train loss: 371.983 - valid loss: 23.164 - train accuracy: 1.0 - valid accuracy: 0.909\n",
      "epoch: 114 - (74 seconds) - train loss: 371.884 - valid loss: 23.933 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 115 - (74 seconds) - train loss: 371.883 - valid loss: 23.76 - train accuracy: 1.0 - valid accuracy: 0.909\n",
      "epoch: 116 - (74 seconds) - train loss: 371.867 - valid loss: 23.57 - train accuracy: 1.0 - valid accuracy: 0.909\n",
      "epoch: 117 - (74 seconds) - train loss: 371.861 - valid loss: 24.047 - train accuracy: 1.0 - valid accuracy: 0.855\n",
      "epoch: 118 - (74 seconds) - train loss: 371.863 - valid loss: 23.938 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 119 - (74 seconds) - train loss: 371.85 - valid loss: 23.386 - train accuracy: 1.0 - valid accuracy: 0.909\n",
      "epoch: 120 - (74 seconds) - train loss: 371.858 - valid loss: 23.859 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 121 - (74 seconds) - train loss: 371.85 - valid loss: 23.734 - train accuracy: 1.0 - valid accuracy: 0.891\n",
      "epoch: 122 - (74 seconds) - train loss: 392.036 - valid loss: 25.102 - train accuracy: 0.959 - valid accuracy: 0.855\n",
      "epoch: 123 - (74 seconds) - train loss: 375.476 - valid loss: 25.51 - train accuracy: 0.994 - valid accuracy: 0.818\n",
      "epoch: 124 - (74 seconds) - train loss: 372.012 - valid loss: 25.378 - train accuracy: 1.0 - valid accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "loss_func = nn.CrossEntropyLoss() # cross entropy loss is used for classification task\n",
    "loss_func_name = 'cross entropy loss'\n",
    "\n",
    "weight_init_pretrain = 'imagenet pretrained'\n",
    "bias_init_pretrain = 'imagenet pretrained'\n",
    "\n",
    "weight_init_other = 'random'\n",
    "bias_init_other = 'random'\n",
    "\n",
    "learn_rate = 0.0001\n",
    "batch_size = 2\n",
    "\n",
    "num_epochs = 125\n",
    "\n",
    "optimizer_name = 'Adam'\n",
    "\n",
    "imagenet_pretrained = False\n",
    "layer_freeze = False\n",
    "frozen_layers = 2 # freeze the first frozen_layers layers in resnet-18\n",
    "\n",
    "# track the start of each hyperparameter run(each run contains num_epochs of epochs)\n",
    "start_time = datetime.now()\n",
    "\n",
    "# overfitting correction hyperparameters\n",
    "#early_stopping_thresh = -1\n",
    "#early_stopping_num_epochs = 7\n",
    "#epoch_train_accu_thresh = 0.95\n",
    "\n",
    "'''Create a unique ID for this hyperparameter run.\n",
    "   It is a folder that all relevent files are saved to (hyperparams file, training logs, model weights, etc.)'''\n",
    "run_id = \"hpt_Adam_34\"\n",
    "os.mkdir(run_id)\n",
    "\n",
    "# record all hyperparameters that might be useful to reference later\n",
    "with open(run_id + '/hyperparams.csv', 'w') as wfil:\n",
    "#    wfil.write(\"note,\" + 'initialized with best weights of hpt_Adam_2' + '\\n')\n",
    "    wfil.write(\"learning rate,\" + str(learn_rate) + '\\n')\n",
    "    wfil.write(\"batch size,\" + str(batch_size) + '\\n')\n",
    "    wfil.write(\"number epochs,\" + str(num_epochs) + '\\n')\n",
    "    \n",
    "    if imagenet_pretrained:\n",
    "        wfil.write(\"weight initialization,\" + weight_init_pretrain + '\\n')\n",
    "        wfil.write(\"bias initialization,\" + bias_init_pretrain + '\\n')\n",
    "    else:\n",
    "        wfil.write(\"weight initialization,\" + weight_init_other + '\\n')\n",
    "        wfil.write(\"bias initialization,\" + bias_init_other + '\\n')\n",
    "    \n",
    "    if imagenet_pretrained:\n",
    "        if layer_freeze:\n",
    "            wfil.write(\"layers frozen,\" + 'first ' + str(frozen_layers) + ' layers' + '\\n')\n",
    "        else:\n",
    "            wfil.write(\"layers frozen,\" + 'NO layer is frozen' + '\\n')\n",
    "    \n",
    "    wfil.write(\"loss function,\" + loss_func_name + '\\n')\n",
    "    wfil.write(\"optimizer,\" + optimizer_name + '\\n')\n",
    "    \n",
    "#    wfil.write(\"early stopping threshold,\" + str(early_stopping_thresh) + '\\n')\n",
    "#    wfil.write(\"early stopping number of epochs necessary,\" + str(early_stopping_num_epochs) + '\\n')\n",
    "#    wfil.write(\"early stopping epoch train accuracy threshold,\" + str(epoch_train_accu_thresh) + '\\n')\n",
    "    \n",
    "    wfil.write(\"start time,\" + str(start_time) + '\\n')\n",
    "\n",
    "# use resnet-18\n",
    "model = models.resnet18(pretrained=imagenet_pretrained)\n",
    "\n",
    "'''\n",
    "modify the model for our classification task:\n",
    "Since there are 4 cell types, the last layer have 4 neurons.\n",
    "'''\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "\n",
    "# use weights from previous tuning and perform further fine tuning\n",
    "# identify where the weights you want to load are \n",
    "#weight_fil = \"hpt_Adam_2/best_weights.pth\"\n",
    "# load weights\n",
    "#model = torch.load(weight_fil)\n",
    "\n",
    "# layer freezing: freeze the first few designated number of layers\n",
    "if layer_freeze:\n",
    "    print(f'The first {frozen_layers} layers are frozen.')\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        if ct < frozen_layers:\n",
    "            #print(child)\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f'Layer {ct+1} is frozen!')\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        ct += 1\n",
    "\n",
    "# This implementation use CUDA for gpu acceleration\n",
    "# check if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    # set the network to use cuda\n",
    "    model = model.cuda()\n",
    "    print(\"CUDA IS AVAILABLE!\")\n",
    "else:\n",
    "    print(\"CUDA NOT AVAILABLE!\")\n",
    "\n",
    "# create loaders to feed data to the network in batches\n",
    "# image size is 640X480, which is larger than 224X224\n",
    "\n",
    "# training dataset loader\n",
    "train_set = mydata('train')\n",
    "trainloader = torch.utils.data.DataLoader(dataset = train_set , batch_size= batch_size , shuffle = True)\n",
    "\n",
    "# validation dataset loader\n",
    "valid_set = mydata('validation')\n",
    "validloader = torch.utils.data.DataLoader(dataset = valid_set , batch_size= batch_size , shuffle = True)\n",
    "\n",
    "# use Adam as optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "# learning rate scheduler\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size =30, gamma=0.1)\n",
    "\n",
    "# track best val loss to know when to save best weights\n",
    "best_valid_loss = \"unset\"\n",
    "\n",
    "# track stuff for early stopping\n",
    "early_stopping_counter = 0\n",
    "es_valid_loss = 0.0\n",
    "\n",
    "with open(run_id + '/log_file.csv', 'w') as log_fil:\n",
    "    # write headers for log file\n",
    "    log_fil.write(\"epoch,epoch duration,train loss,valid loss,train accuracy,valid accuracy\\n\")\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "                       \n",
    "        epoch_start = datetime.now()\n",
    "        \n",
    "        # track train and validation loss\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_valid_loss = 0.0\n",
    "        \n",
    "        # track train and validation accuracy\n",
    "        epoch_train_accuracy = 0.0\n",
    "        epoch_valid_accuracy = 0.0\n",
    "        \n",
    "        for i, (train_images, train_labels) in enumerate(trainloader):\n",
    "            #print(f'batch {i}, time {datetime.now()}')\n",
    "            # for each batch, set images and labels to be cuda compatible\n",
    "            train_images = Variable(train_images).cuda()\n",
    "            train_labels = Variable(train_labels).cuda()\n",
    "\n",
    "            # zero out gradients for every batch or they will accumulate\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward step\n",
    "            train_outputs = model(train_images)\n",
    "            # sigmoid activation for output layer to match cross entropy loss funciton\n",
    "            train_outputs = torch.sigmoid(train_outputs)\n",
    "            \n",
    "            # for loss function, convert the shape of outputs and labels to match\n",
    "            #train_outputs = train_outputs.reshape(train_outputs.shape[0])\n",
    "            #train_labels  = train_labels.reshape(train_labels.shape[0])\n",
    "                        \n",
    "            # compute loss\n",
    "            train_loss = loss_func(train_outputs, train_labels)\n",
    "            #print(f'train loss for batch {i} is {train_loss}')\n",
    "             \n",
    "            # backwards step\n",
    "            train_loss.backward()         \n",
    "            \n",
    "            # update weights and biases\n",
    "            optimizer.step()\n",
    "            \n",
    "            # track training loss\n",
    "            epoch_train_loss += train_loss.item()\n",
    "            \n",
    "            # get training accuracy\n",
    "            for i, image_label in enumerate(train_outputs):\n",
    "                temp_max, temp_index = image_label.max(0) # the index of the max value for each image's output is the image's class\n",
    "                #print(f'train output is {image_label}')\n",
    "                #print(f'Output label is {temp_index}, true label is {train_labels[j]}. They are same: {temp_index==train_labels[j]}')\n",
    "                if temp_index==train_labels[i]:\n",
    "                    epoch_train_accuracy += 1\n",
    "        \n",
    "        epoch_train_accuracy = epoch_train_accuracy/len(train_set)\n",
    "        \n",
    "        # learning rate scheduler\n",
    "        #scheduler.step()\n",
    "        \n",
    "        # track valid loss - the torch.no_grad() ensures gradients will not be updated based on validation set\n",
    "        with torch.no_grad():\n",
    "            for i, (valid_images, valid_labels) in enumerate(validloader):\n",
    "                # for each batch, set images and labels to be cuda compatible\n",
    "                valid_images = Variable(valid_images).cuda()\n",
    "                valid_labels = Variable(valid_labels).cuda()\n",
    "\n",
    "                valid_outputs = model(valid_images)\n",
    "                valid_outputs = torch.sigmoid(valid_outputs)\n",
    "                # for loss function, convert the shape of outputs and labels to match\n",
    "                #valid_outputs = valid_outputs.reshape(valid_outputs.shape[0])\n",
    "                #valid_labels  = valid_labels.reshape(valid_labels.shape[0])\n",
    "            \n",
    "                valid_loss = loss_func(valid_outputs, valid_labels)\n",
    "                # track validation loss\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "                \n",
    "                # get validation accuracy\n",
    "                for i, image_label in enumerate(valid_outputs):\n",
    "                    temp_max, temp_index = image_label.max(0)\n",
    "                    if temp_index==valid_labels[i]:\n",
    "                        epoch_valid_accuracy += 1\n",
    "                \n",
    "        epoch_valid_accuracy = epoch_valid_accuracy/len(valid_set)\n",
    "            \n",
    "           \n",
    "                                        \n",
    "        # track total epoch time\n",
    "        epoch_end = datetime.now()\n",
    "        epoch_time = (epoch_end - epoch_start).total_seconds()\n",
    "        \n",
    "        # save best weights\n",
    "        if (best_valid_loss==\"unset\") or (epoch_valid_loss < best_valid_loss):\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            torch.save(model, run_id + \"/best_weights.pth\")\n",
    "        \n",
    "        # save most recent weights\n",
    "        torch.save(model, run_id + \"/last_weights.pth\")\n",
    "        \n",
    "        # save epoch results in log file\n",
    "        log_fil.write(str(epoch) + ',' +\n",
    "                      str(epoch_time) + ',' +\n",
    "                      str(epoch_train_loss) + ',' + \n",
    "                      str(epoch_valid_loss) + ',' + \n",
    "                      str(epoch_train_accuracy) + ',' +\n",
    "                      str(epoch_valid_accuracy) + '\\n')\n",
    "        \n",
    "        # print out epoch level training details\n",
    "        print(\"epoch: \" + str(epoch) + \" - (\"+ str(round(epoch_time)) + \" seconds)\" +\n",
    "              \" - train loss: \" + str(round(epoch_train_loss, 3)) +\n",
    "              \" - valid loss: \" + str(round(epoch_valid_loss, 3)) + \n",
    "              \" - train accuracy: \" + str(round(epoch_train_accuracy, 3)) + \n",
    "              \" - valid accuracy: \" + str(round(epoch_valid_accuracy, 3)))\n",
    "\n",
    "'''\n",
    "        # implement early stopping\n",
    "        if es_valid_loss == 0.0:\n",
    "            early_stopping_counter = 0\n",
    "            es_valid_loss = epoch_valid_loss\n",
    "            \n",
    "        if es_valid_loss - epoch_valid_loss < early_stopping_thresh:\n",
    "            early_stopping_counter += 1\n",
    "        else:\n",
    "            early_stopping_counter = 0\n",
    "            es_valid_loss = epoch_valid_loss\n",
    "        \n",
    "        if (early_stopping_counter >= early_stopping_num_epochs):\n",
    "            print(\"Stopped early by continuous increase of validation loss.\")\n",
    "            break\n",
    "            \n",
    "        \n",
    "        if  (epoch_train_accuracy > epoch_train_accu_thresh):\n",
    "            print('Stopped early by reaching epoch train accuracy threshold.')\n",
    "            break\n",
    "'''\n",
    "\n",
    "end_time = datetime.now()\n",
    "with open(run_id + '/hyperparams.csv', 'a') as wfil:\n",
    "    wfil.write(\"end time,\" + str(end_time) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS AVAILABLE!\n"
     ]
    }
   ],
   "source": [
    "# Inference for validation dataset after tuning\n",
    "\n",
    "# identify where the weights you want to load are \n",
    "weight_fil = \"hpt_Adam_34/best_weights.pth\"\n",
    "es_epoch = 125\n",
    "\n",
    "# set necessary hyperparameters\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "batch_size = 2\n",
    "\n",
    "# load weights\n",
    "model = torch.load(weight_fil)\n",
    "\n",
    "# put model in evaluation mode (sets dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results)\n",
    "model.eval()\n",
    "\n",
    "# This implementation use CUDA for gpu acceleration\n",
    "# check if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    # set the network to use cuda\n",
    "    model = model.cuda()\n",
    "    print(\"CUDA IS AVAILABLE!\")\n",
    "else:\n",
    "    print(\"CUDA NOT AVAILABLE!\")\n",
    "\n",
    "# create loaders to feed data to the network in batches\n",
    "# image size is 640X480, which is larger than 224X224\n",
    "\n",
    "# training dataset loader\n",
    "eval_set = mydata('validation')\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(dataset = eval_set , batch_size= batch_size , shuffle = False)\n",
    "\n",
    "\n",
    "# track metrics over dataset\n",
    "eval_loss = 0.0\n",
    "eval_accuracy = 0.0\n",
    "\n",
    "# track true label and predicted label for each image\n",
    "image_names = eval_set.image_names\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# loop through eval data\n",
    "for i, (images, labels) in enumerate(eval_loader):\n",
    "    \n",
    "    # for each batch, set images and labels to be cuda compatible\n",
    "    images = Variable(images).cuda()\n",
    "    labels = Variable(labels).cuda()\n",
    "    \n",
    "    # run the model on the eval batch\n",
    "    outputs = model(images)\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    \n",
    "    # compute eval loss\n",
    "    loss = loss_func(outputs, labels)\n",
    "    # track loss for testing dataset\n",
    "    eval_loss += loss.item()\n",
    "    \n",
    "    # get testing accuracy\n",
    "    for i, image_label in enumerate(outputs):\n",
    "        temp_max, temp_index = image_label.max(0)\n",
    "        \n",
    "        true_labels.append(labels[i])\n",
    "        predicted_labels.append(temp_index)\n",
    "        \n",
    "        if temp_index==labels[i]:\n",
    "            eval_accuracy += 1\n",
    "                \n",
    "eval_accuracy = eval_accuracy/len(eval_set)\n",
    "\n",
    "# save the inference accuracy\n",
    "print(f'Early stop (epoch {es_epoch}, {weight_fil}): tuned accuracy for validation dataset is {eval_accuracy}',\n",
    "      file=open(\"tuned_result_valid.txt\", \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of Testing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA IS AVAILABLE!\n"
     ]
    }
   ],
   "source": [
    "# Inference for testing dataset after tuning\n",
    "\n",
    "# identify where the weights you want to load are \n",
    "weight_fil = \"hpt_Adam_29/best_weights.pth\"\n",
    "\n",
    "# set necessary hyperparameters\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "batch_size = 2\n",
    "\n",
    "# load weights\n",
    "model = torch.load(weight_fil)\n",
    "\n",
    "# put model in evaluation mode (sets dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results)\n",
    "model.eval()\n",
    "\n",
    "# This implementation use CUDA for gpu acceleration\n",
    "# check if CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    # set the network to use cuda\n",
    "    model = model.cuda()\n",
    "    print(\"CUDA IS AVAILABLE!\")\n",
    "else:\n",
    "    print(\"CUDA NOT AVAILABLE!\")\n",
    "\n",
    "# create loaders to feed data to the network in batches\n",
    "# image size is 640X480, which is larger than 224X224\n",
    "\n",
    "# training dataset loader\n",
    "eval_set = mydata('test')\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(dataset = eval_set , batch_size= batch_size , shuffle = False)\n",
    "\n",
    "\n",
    "# track metrics over dataset\n",
    "eval_loss = 0.0\n",
    "eval_accuracy = 0.0\n",
    "\n",
    "# track true label and predicted label for each image\n",
    "image_names = eval_set.image_names\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# loop through eval data\n",
    "for i, (images, labels) in enumerate(eval_loader):\n",
    "    \n",
    "    # for each batch, set images and labels to be cuda compatible\n",
    "    images = Variable(images).cuda()\n",
    "    labels = Variable(labels).cuda()\n",
    "    \n",
    "    # run the model on the eval batch\n",
    "    outputs = model(images)\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    \n",
    "    # compute eval loss\n",
    "    loss = loss_func(outputs, labels)\n",
    "    # track loss for testing dataset\n",
    "    eval_loss += loss.item()\n",
    "    \n",
    "    # get testing accuracy\n",
    "    for i, image_label in enumerate(outputs):\n",
    "        temp_max, temp_index = image_label.max(0)\n",
    "        \n",
    "        true_labels.append(labels[i])\n",
    "        predicted_labels.append(temp_index)\n",
    "        \n",
    "        if temp_index==labels[i]:\n",
    "            eval_accuracy += 1\n",
    "                \n",
    "eval_accuracy = eval_accuracy/len(eval_set)\n",
    "            \n",
    "# save the inference loss and accuracy\n",
    "print(f'{weight_fil}: \\nTuned loss for testing dataset is {eval_loss}.\\nTuned accuracy for testing dataset is {eval_accuracy}.\\n',\n",
    "      file=open(\"tuned_result.txt\", \"a\"))\n",
    "\n",
    "\n",
    "# save the inferenced labels and true labels\n",
    "infer_list = list(zip(image_names, true_labels, predicted_labels))                 \n",
    "infer_df   = pd.DataFrame(infer_list, columns = ['image name', 'true label', 'predicted label'])\n",
    "infer_df.to_csv('inference_result_29_test.csv', header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
